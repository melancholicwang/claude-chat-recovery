#!/usr/bin/env python3
"""ç”Ÿæˆä¼šè¯ç»Ÿè®¡æ‘˜è¦"""

import json
import sys

def analyze_jsonl(file_path):
    stats = {
        'total_lines': 0,
        'queue_operations': 0,
        'user_messages': 0,
        'assistant_messages': 0,
        'tool_uses': 0,
        'tool_results': 0,
        'thinking_blocks': 0,
        'text_responses': 0,
        'unique_message_ids': set(),
        'total_input_tokens': 0,
        'total_output_tokens': 0,
        'total_cache_read_tokens': 0,
    }
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            stats['total_lines'] += 1
            try:
                obj = json.loads(line.strip())
                obj_type = obj.get('type')
                
                if obj_type == 'queue-operation':
                    stats['queue_operations'] += 1
                elif obj_type == 'user':
                    stats['user_messages'] += 1
                    content = obj.get('message', {}).get('content', [])
                    for item in content:
                        if item.get('type') == 'tool_result':
                            stats['tool_results'] += 1
                elif obj_type == 'assistant':
                    stats['assistant_messages'] += 1
                    message = obj.get('message', {})
                    msg_id = message.get('id')
                    if msg_id:
                        stats['unique_message_ids'].add(msg_id)
                    
                    # ç»Ÿè®¡usage
                    usage = message.get('usage', {})
                    if usage:
                        stats['total_input_tokens'] += usage.get('input_tokens', 0)
                        stats['total_output_tokens'] += usage.get('output_tokens', 0)
                        stats['total_cache_read_tokens'] += usage.get('cache_read_input_tokens', 0)
                    
                    # ç»Ÿè®¡contentç±»å‹
                    content = message.get('content', [])
                    for item in content:
                        item_type = item.get('type')
                        if item_type == 'thinking':
                            stats['thinking_blocks'] += 1
                        elif item_type == 'text':
                            stats['text_responses'] += 1
                        elif item_type == 'tool_use':
                            stats['tool_uses'] += 1
                            
            except json.JSONDecodeError:
                continue
    
    stats['unique_message_ids'] = len(stats['unique_message_ids'])
    return stats

if __name__ == '__main__':
    file_path = sys.argv[1] if len(sys.argv) > 1 else 'case.jsonl'
    stats = analyze_jsonl(file_path)
    
    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " " * 18 + "ä¼šè¯ç»Ÿè®¡æ‘˜è¦" + " " * 24 + "â•‘")
    print("â•š" + "â•" * 58 + "â•")
    print()
    print(f"ğŸ“„ æ–‡ä»¶: {file_path}")
    print(f"ğŸ“Š æ€»è¡Œæ•°: {stats['total_lines']}")
    print()
    print("æ¶ˆæ¯ç»Ÿè®¡:")
    print(f"  â€¢ é˜Ÿåˆ—æ“ä½œ: {stats['queue_operations']}")
    print(f"  â€¢ ç”¨æˆ·æ¶ˆæ¯è¡Œ: {stats['user_messages']}")
    print(f"  â€¢ åŠ©æ‰‹æ¶ˆæ¯è¡Œ: {stats['assistant_messages']}")
    print(f"  â€¢ å”¯ä¸€åŠ©æ‰‹æ¶ˆæ¯æ•°: {stats['unique_message_ids']}")
    print()
    print("å†…å®¹ç»Ÿè®¡:")
    print(f"  â€¢ ğŸ’­ æ€è€ƒå—: {stats['thinking_blocks']}")
    print(f"  â€¢ ğŸ’¬ æ–‡æœ¬å›å¤: {stats['text_responses']}")
    print(f"  â€¢ ğŸ”§ å·¥å…·è°ƒç”¨: {stats['tool_uses']}")
    print(f"  â€¢ ğŸ“¤ å·¥å…·ç»“æœ: {stats['tool_results']}")
    print()
    print("Token ç»Ÿè®¡:")
    print(f"  â€¢ è¾“å…¥: {stats['total_input_tokens']:,}")
    print(f"  â€¢ è¾“å‡º: {stats['total_output_tokens']:,}")
    print(f"  â€¢ ç¼“å­˜è¯»å–: {stats['total_cache_read_tokens']:,}")
    print(f"  â€¢ æ€»è®¡: {stats['total_input_tokens'] + stats['total_output_tokens']:,}")
